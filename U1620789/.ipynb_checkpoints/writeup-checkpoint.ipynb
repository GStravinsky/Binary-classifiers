{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions package and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from scipy import interp\n",
    "from sklearn.metrics import plot_precision_recall_curve\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib as matplotlib\n",
    "import seaborn as sns\n",
    "from numpy import loadtxt\n",
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "Xtest = np.loadtxt(\"Xtest.csv\", delimiter=' ')\n",
    "Xtrain = np.loadtxt(\"Xtrain.csv\", delimiter=' ' )\n",
    "Ytrain = np.loadtxt(\"Ytrain.csv\", delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ROC-AUC, accuracy and AUC-PR metrics in one\n",
    "\n",
    "def evaluation_metrics(X, classifier, print_summary = True, has_decision_function = False):\n",
    "    # INPUT: \n",
    "    # X - the design matrix.\n",
    "    # classifier - the predefined classifier/model\n",
    "    # print_summary - a concise mean, standard deviation report will be returned\n",
    "    #                 including the results for all the folds.\n",
    "    # has_decision_function - probabilistic models such as perceptron, SVM in sklearn have \n",
    "    #                 inbuilt decision_function to define threshold. It is prefered to use that.\n",
    "    # OUTPUT:\n",
    "    # 2 dictionaries for means and standard deviations of all the metrics taken across all folds.\n",
    "    \n",
    "    # lists to contain the results for all 3 classifiers\n",
    "    roc =[]\n",
    "    pr = []\n",
    "    accuracy = []\n",
    "   \n",
    "    for i, (train, test) in enumerate(skf.split(X, Ytrain)):\n",
    "        classifier.fit(X[train], Ytrain[train])\n",
    "        \n",
    "        # checking which attribute to use to define the threshold\n",
    "        # using \"predict_proba\" or \"decision_function\" for a more accurate evaluation than\n",
    "        # with the crude predicted scores obtained with \"predict\".\n",
    "        if has_decision_function == False:\n",
    "            probs = classifier.predict_proba(X[test])\n",
    "            preds = probs[:,1]\n",
    "        elif has_decision_function == True:\n",
    "            preds = classifier.decision_function(X[test])\n",
    "        \n",
    "        # calculating ROC-AUC score\n",
    "        fpr,tpr,tt = roc_curve(Ytrain[test], preds)\n",
    "        roc_score = auc(fpr, tpr)\n",
    "        \n",
    "        # calculating PR-AUC score\n",
    "        pr_score = average_precision_score(Ytrain[test], preds)\n",
    "        \n",
    "        # manually defining accuracy\n",
    "        accuracy_score = np.mean(Ytrain[test] == classifier.predict(X[test]))\n",
    "        \n",
    "        # storing the values for each fold\n",
    "        roc.append(roc_score)\n",
    "        pr.append(pr_score)\n",
    "        accuracy.append(accuracy_score)\n",
    "    \n",
    "    # creating dictionaries for means and standard deviations of the performance metrics\n",
    "    means = {'roc':np.mean(roc),'pr': np.mean(pr),'accuracy':np.mean(accuracy)}\n",
    "    stds ={'roc': np.std(roc),'pr': np.std(pr),'accuracy':np.std(accuracy)}\n",
    "    \n",
    "    # a \"wordy\" summary\n",
    "    if print_summary == True:\n",
    "        print('ROC-AUC scores for each fold;', roc)\n",
    "        print(\"ROC-AUC: %0.2f (+/- %0.2f)\" % (np.mean(roc), np.std(roc) * 2))\n",
    "        print('PR-AUC scores for each fold;', pr)\n",
    "        print(\"AUC-PR: %0.2f (+/- %0.2f)\" % (np.mean(pr), np.std(pr) * 2))\n",
    "        print('Accuracy score for each fold;',accuracy)\n",
    "        print(\"Accuracy: %0.2f (+/- %0.2f)\" % (np.mean(accuracy), np.std(accuracy) * 2))\n",
    "        \n",
    "    return means, stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "### Showing the data\n",
    "\n",
    "There are 3000 rows (images) of training data with corespoinding 3000 labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(Xtrain.shape)\n",
    "print(Ytrain.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images depict clothing, shoes, purses. The classification rule is not clear for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating sub-variables to show 1 and -1 labelled images\n",
    "pos = Xtrain[Ytrain == 1]\n",
    "neg = Xtrain[Ytrain == -1]\n",
    "\n",
    "# can change the range if wanted.\n",
    "for i in range(10,12): \n",
    "    image_pos = np.reshape(pos[i], (28,28))\n",
    "    image_neg = np.reshape(neg[i], (28,28))\n",
    "    # images will alternative between positive and negative labelled ones\n",
    "    plt.matshow(image_pos)\n",
    "    plt.matshow(image_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positive/ negative count \n",
    "\n",
    "There are 1821 negatives and 1179 positive labels. Thus, the data is moderately imbalanced with a rough 60/40 split (dominated by negatives)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(Ytrain, return_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choice of a performance metric\n",
    "\n",
    "\n",
    "Due to the imbalance of the data, accuracy seems to be not the most appropriate performace evaluation tool owing to its bias toward the dominant category. Thus, AUC-ROC or AUC-PR are prefered options. The choice among these two metrics depends on the severity of the imbalance and the weight we put on correctly predicting the positive class (and thus precision). Since (1) the skew of the data is not severe (60/40), (2) no information is given about the goals of prediction and the fact that (3) the nature of the data (clothing) does not suggest that False Positives or False Negatives are more costly, I decided to use AUC-ROC as the performace evaluation metric. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random classifier accuracy\n",
    "\n",
    "In general, a random classifier is expected to predict 50% of the labels correctly, yet we could derive a slightly more sphisticated expectation of the random classifier which would allow to incorporate different rates of predicting a certain group (predicting everything is of one class is as random as predicting 50/50 split). Accuracy is defined as a ratio of correctly labeled instances (True positives + True negetives) to the total population. To calculate expected confussion matrix terms, let's define $q$ as a probability that a random classifier assigns a positive label to a random input. In addition, recall that our training data has 60% of negative and 40% of positive indtances. Then the expected proportions of all the confussion matrix terms are:\n",
    "\n",
    "$TP = 0.4q \\\\\n",
    "TN = (1-q)0.6 \\\\\n",
    "FP = 0.6q\\\\\n",
    "FN = (1-q)0.4$\n",
    "\n",
    "Hence the expected accuracy of a random classifier is $TP+TN = 0.6-0.2q$, which for $q=0.5$ results in accuracy being 50%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â ROC-AUC and PR-AUC for a random classifier\n",
    "\n",
    "AUC-ROC curve is defined as the area under the true positive rate (TPR) against the false positive rate (FPR) curve. \n",
    "\n",
    "$TPR = \\frac{TP}{FN+TP}= \\frac{0.4q}{(1-q)0.4+0.4}=q \\\\\n",
    "FPR = \\frac{FP}{FP+TN}= \\frac{0.6q}{(1-q)0.6+0.6}=q$\n",
    "\n",
    "Hence the expectation of TPR over the range of FPR is:\n",
    "\n",
    "$\\int_0^1qdq=\\frac{q^2}{2}|_0^1=1/2$\n",
    "\n",
    "And since the expected AUC-ROC does not depend on the proportion of the data, the same result holds for the testing data.\n",
    "\n",
    "AUC-PR, on the other hand is defined as the are under the precision against TPR curve. For the testing data is follows that:\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}=\\frac{0.4q}{0.4q+0.6q}=0.4$\n",
    "\n",
    "Hence the expected training AUC-PR is\n",
    "\n",
    "$\\int_0^10.4qd=0.4q|_0^1 = 0.4$\n",
    "\n",
    "By the same reasoning, it follows that expected random classifier AUC-PR for the testing data would be equal to the proportion of positive items in a data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "\n",
    "After fitting the model, I am plotting ROC and PR curves that also lists the corresponding AUC scores for each of 5 folds. Following that, I give a more concise report of all the three performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model for k=1 together with the Stratified 5-folds used in all the assignment.\n",
    "sknn = KNeighborsClassifier(n_neighbors=1)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance of KNN with K=1 classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-ROC for each fold. sklearn 22.1 or later required\n",
    "# AUC-ROC with plots.\n",
    "aucs = []\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "for i, (train, test) in enumerate(skf.split(Xtrain, Ytrain)):\n",
    "    sknn.fit(Xtrain[train], Ytrain[train])\n",
    "    # requires sklearn 22.1\n",
    "    viz = plot_roc_curve(sknn, Xtrain[test], Ytrain[test],\n",
    "                         name='ROC fold {}'.format(i),\n",
    "                         alpha=0.3, lw=1, ax=ax)\n",
    "    aucs.append(viz.roc_auc) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC-PR curve\n",
    "# requires sklearn 22.1\n",
    "fig, ax = plt.subplots()\n",
    "for i, (train, test) in enumerate(skf.split(Xtrain, Ytrain)):\n",
    "    sknn.fit(Xtrain[train], Ytrain[train])\n",
    "    viz = plot_precision_recall_curve(sknn, Xtrain[test], Ytrain[test],\n",
    "                         name='PR fold {}'.format(i),\n",
    "                         alpha=0.3, lw=1, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more concise report of ROC-AUC, PR-AUC and accuracy scores for each fold is printed bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting summary for all 3 metrics using own-made function.\n",
    "\n",
    "base_score, base_stds = evaluation_metrics(Xtrain, sknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance summary for KNN, K=1\n",
    "Using previously used function, I list the mean and standard deviation of each performance measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean of the evaluation metrics for KNN the k = 1,  5 Stratified folds\n",
    "base_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard deviation for KNN the k = 1,  5 Stratified folds\n",
    "base_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC score reaches 0.747 with a relativelly low standard deviation of 0.015. So there is definintelly an increase in predictive power over the random classifier. Yet, the error is still huge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am implementing the following transformations: scaling, scaling to a range (between 0:1 and -1:1), robust scaling, mapping to uniform and Gaussian distributions and normalization. Sparse data transformations is not applied because the data is not sparse - all needed values are present. Robust scaling, albeit not neccesary due to the the abscence of strong outliers, is still conducted to deduce the importance of extreme values. I only elaborate more about the highest performing preprocessing methods - scaling, mapping to uniform distribution and normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling\n",
    "scaling = preprocessing.StandardScaler().fit(Xtrain)\n",
    "Xscaled = scaling.transform(Xtrain)\n",
    "\n",
    "scaled_scores, scaled_stds = evaluation_metrics(Xscaled, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_scores\n",
    "# 0.7523 ROC-AUC. Slight improvement from the raw data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MinMax Scaler - scaling values to [0:1] range\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "Xtrain_minmax = min_max_scaler.fit_transform(Xtrain)\n",
    "\n",
    "minmax_scores, _ = evaluation_metrics(Xtrain_minmax, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minmax_scores\n",
    "# 0.7457 ROC-AUC. Not much improvement from the raw data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MaxAbs Scaler - centerint to [-1:1]\n",
    "# Must be centered at zero a priori\n",
    "\n",
    "demeaning = preprocessing.StandardScaler(with_std=False).fit(Xtrain)\n",
    "Xdemeaned = demeaning.transform(Xtrain)\n",
    "max_abs_scaler = preprocessing.MaxAbsScaler()\n",
    "Xtrain_maxabs = max_abs_scaler.fit_transform(Xdemeaned)\n",
    "\n",
    "maxabs_scores, _ = evaluation_metrics(Xtrain_maxabs, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxabs_scores\n",
    "# 0.7427 ROC-AUC. Worse than the raw data case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Scaling - takes outliers into account better\n",
    "robust_scaling = preprocessing.RobustScaler().fit(Xtrain)\n",
    "Xrobust = robust_scaling.transform(Xtrain)\n",
    "\n",
    "robust_scores, _ = evaluation_metrics(Xrobust, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "robust_scores\n",
    "# 0.7325 ROC-AUC. Worse than the raw data - more extreme values are informative of classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uniform distribution transformation\n",
    "\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state=0)\n",
    "Xuniform = quantile_transformer.fit_transform(Xtrain)\n",
    "\n",
    "uniform_scores, uniform_stds = evaluation_metrics(Xuniform, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniform_scores\n",
    "# 0.7555 ROC-AUC. Slight improvement from the raw data case in all the measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_transformer = preprocessing.QuantileTransformer(output_distribution='normal', random_state=0)\n",
    "Xnormal = normal_transformer.fit_transform(Xtrain)\n",
    "\n",
    "normal_scores, _ = evaluation_metrics(Xnormal, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_scores\n",
    "# 0.6602 ROC-AUC. Significantly worse than the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making each row to be unit lenght by L2 norm.\n",
    "Xnormalized = preprocessing.normalize(Xtrain, norm='l2')\n",
    "\n",
    "normalized_scores, normalized_stds = evaluation_metrics(Xnormalized, sknn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_scores\n",
    "# 0.7546 ROC-AUC. Slightly better than the raw data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis only scaling, normalization and mapping to uniform distribution succeeded in slighltly improving ROC-AUC score. I cover all three of these preprocessing methods in slightly more detail.\n",
    "\n",
    "1. Scaling - the simpliest transformation. The data is column-centred and each column is devided by its standard deviation.\n",
    "\n",
    "2. Mapping to uniform - a slightly trickier preprocessing method. This preprocessing takes all data and transforms accoring to the quantiles of the uniform distribution - thus all the values are contained between 0 and 1.\n",
    "\n",
    "3. Normalization - a standard and simple way of processing data. Each observation, or in other words row, of the dataset is transformed so as to have a unit L2 norm.\n",
    "\n",
    "The concise comparison table is available bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_data = { \" \": [\"Scaling\", \"Map to uniform\", \"Normalization\"],\n",
    "                    \"ROC-AUC\": [scaled_scores['roc'], uniform_scores['roc'], normalized_scores['roc']],\n",
    "                      \"ROC-AUC s.d.\": [scaled_stds['roc'], uniform_stds['roc'], normalized_stds['roc']],\n",
    "                     \"PR-AUC\": [scaled_scores['pr'], uniform_scores['pr'], normalized_scores['pr']],\n",
    "                     \"PR-AUC s.d.\": [scaled_stds['pr'], uniform_stds['pr'], normalized_stds['pr']],\n",
    "                     \"Accuracy\": [scaled_scores['accuracy'], uniform_scores['accuracy'], normalized_scores['accuracy']],\n",
    "                      \"Accuracy s.d.\": [scaled_stds['accuracy'], uniform_stds['accuracy'], normalized_stds['accuracy']],\n",
    "                     }\n",
    "\n",
    "preprocessing_summary = pd.DataFrame(preprocessing_data, columns = [' ','ROC-AUC','ROC-AUC s.d.','PR-AUC','PR-AUC s.d.', 'Accuracy','Accuracy s.d.'])\n",
    "\n",
    "preprocessing_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the table above we see that when it comes to maximisin ROC-AUC, mapping to uniform distribution does the best job. Besides that, mapping to uniform distribution achieves the lowest standard deviation. However, normalization delivers a performance very similar to that of uniform mapping, yet with slightly lower score and higer standard deviation. Scaling, on the other hand gives the lowest ROC-AUC. Normalization is a rather simple and intuitive preprocessing of the image data. In addition, it is a recommended pre-processing method for classifiers such as Support Vector Machines and XGBoosting which will be implemented later. On the other hand, mapping to uniform distribution, albeit simple, is a less intuitive way of transforming image data. In addition, testing both uniformly mapped and normalized data on higher K KNN classifiers, unanimously showed that normalization is a preferrd method (analysis will not be reported here). Thus, for the above reasons, I decided to proceed with normalized data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Choosing optimal K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating odd list of K for KNN\n",
    "neighbors = list(range(1, 50, 2))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores = []\n",
    "cv_std = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    # Using the same function I made to get all three evaluation metrics\n",
    "    means, stds = evaluation_metrics(X = Xnormalized, classifier = knn, print_summary = False)\n",
    "    means['k'] = k\n",
    "    stds['k'] = k\n",
    "    cv_scores.append(means)\n",
    "    cv_std.append(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dataframe to analyse optimal k\n",
    "normalized_k = pd.DataFrame(cv_scores)\n",
    "normalized_k_stds = pd.DataFrame(cv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the Ks against the ROC-AUC performance.\n",
    "normalized_k.plot.scatter(y='roc', x = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding the best k according to roc\n",
    "normalized_k.loc[normalized_k['roc'] == normalized_k['roc'].max(), 'k']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking Ks against ROC standard deviation\n",
    "normalized_k_stds.plot.scatter(y='roc', x = 'k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking PR-AUC curve score against Ks\n",
    "normalized_k.plot.scatter(y='pr', x = 'k')\n",
    "\n",
    "# follows ROC_AUC, as it should"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above analysis it seems that the ROC-AUC and PR-AUC scores increase substantially in the range between k=1 and k=10 and then slow down. According to ROC-AUC the highest score is achieved at K=23. The standard deviation at this point is rather average and moderate overall. \n",
    "\n",
    "I was using 2-stop interval before. Now I want to zoom in to the highest range of ROC-AUC score and use 1-stop interval to make sure the K I am choosing is actually maximum for a given measure. No higer Ks will be checked because the ROC-AUC curve suggest a steady decrease with Ks above 30."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking narrower region of Ks using 1-stops now.\n",
    "\n",
    "# creating odd list of K for KNN\n",
    "neighbors = list(range(10, 26, 1))\n",
    "\n",
    "# empty list that will hold cv scores\n",
    "cv_scores_zoom = []\n",
    "cv_std_zoom = []\n",
    "\n",
    "for k in neighbors:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    means, stds = evaluation_metrics(X = Xnormalized, classifier = knn, print_summary = False)\n",
    "    means['k'] = k\n",
    "    stds['k'] = k\n",
    "    cv_scores_zoom.append(means)\n",
    "    cv_std_zoom.append(stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe to analyze Ks\n",
    "normalized_k_zoom = pd.DataFrame(cv_scores_zoom)\n",
    "normalized_k_zoom_std = pd.DataFrame(cv_std_zoom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Ks against the ROC-AUC score\n",
    "normalized_k_zoom.plot.scatter(y='roc', x ='k')\n",
    "# 14 is the highest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the ROC-AUC variance for diffeent K values\n",
    "normalized_k_zoom_std.plot.scatter(y='roc', x ='k')\n",
    "# even visually k=14 seen that k=14 has lower variance than the closest contenders k=21, k=23"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the k with the highest ROC-AUC\n",
    "normalized_k_zoom.loc[normalized_k_zoom['roc'] == normalized_k_zoom['roc'].max(), 'k']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the analysis above, it seems that K=14 has the higer ROC-AUC (0.8449) score with lower variance than previously used K=23 (0.8440). Thus, I choose K=14 as an optimal K for KNN. The performance evaluation for K=14 is given bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy, AUC-ROC and AUC-PR for k=14\n",
    "normalized_k_zoom.loc[normalized_k_zoom['k'] == 14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing perceptron with normalized X.\n",
    "\n",
    "perceptron = Perceptron(random_state = 10)\n",
    "\n",
    "perceptron_scores, perceptron_stds = evaluation_metrics(X=Xnormalized, classifier = perceptron, \n",
    "                                                        has_decision_function = True)\n",
    "# 0.77 ROC-AUC. Significantly worse performance than with KNN, K=14. \n",
    "# Note: there is a slight random_state dependence, but the score is still low. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying uniformly mapped data instead with the perceptron\n",
    "perceptron_scores, perceptron_stds = evaluation_metrics(X=Xuniform, classifier = perceptron,\n",
    "                                                       has_decision_function = True)\n",
    "# 0.72 ROC-AUC, Even worse results than with the normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying perceptron with scaled data\n",
    "\n",
    "perceptron_scores, perceptron_stds = evaluation_metrics(X=Xscaled, classifier = perceptron,\n",
    "                                                       has_decision_function = True)\n",
    "# 0.72 ROC-AUC. Huge standard errors. Poor performance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall perceptron gives inferior results to KNN which is expected due to non-linearity of the problem. The highest ROC-AUC score achieved with perceptron is the one with the normalized data (0.77)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Support Vector Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by running a grid search for the best parameters of Linear SVC. I choose completelly arbitrary parameters aiming to covers full parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search method to choose the most optimal C for Linear SVC \n",
    "\n",
    "param_linear = [{'kernel': ['linear'], 'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "svc = SVC(class_weight = 'balanced')\n",
    "clf = GridSearchCV(svc, param_linear, scoring = 'roc_auc')\n",
    "clf.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the breakdown of all the results.\n",
    "res_linear = pd.DataFrame(clf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the best parameters and scores for the Linear SVC\n",
    "print(\"Best Parameters:\", clf.best_params_, \"ROC-AUC:\", clf.best_score_ )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSVC = SVC(class_weight='balanced', C = 1, kernel= 'linear')\n",
    "\n",
    "LSVC_score, LSVC_std = evaluation_metrics(Xnormalized, classifier=LSVC, has_decision_function=True)\n",
    "# 0.79 ROC-AUC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equal weight between the margin width and the correctness of classification (C=1) gave the highest result for Linear Support Vector Classifier (0.79 ROC-AUC). The score, however, is not good and is much worse than that obtained with KNN, K=14. This echoes that the classification problem is non-linear (as is obvious from the pictures of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by running a grid search for the best parameters of SVM. I choose completelly arbitrary parameters aiming to covers full parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search method to choose the most optimal kernel and parameters for SVM \n",
    "\n",
    "param_kernalized = [{'kernel': ['poly'], 'degree': [2, 3, 4, 5], 'C': [0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['rbf'], 'gamma': [1e-2, 1e-4, 1, 10], 'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "clf_kernalized = GridSearchCV(svc, param_kernalized, scoring = 'roc_auc')\n",
    "clf_kernalized.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the breakdown of all the results.\n",
    "res_kernalized = pd.DataFrame(clf_kernalized.cv_results_)\n",
    "res_kernalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Printing the best parameters and corresponding score.\n",
    "print(clf_kernalized.best_params_, \"ROC-AUC:\", clf_kernalized.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF kernel with $\\gamma = 10$ and C = 10 gave the highest results. From the results table above we can see that top 3 results were RBF with $\\gamma = 10$. The ROC-AUC score of 0.8759 is the highest so far, exceeding that of KNN, K=14.\n",
    "\n",
    "I did another Grid Search this time with higher $\\gamma$ values specified. The optimal parameters remained the same. The analysis is available bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retuning gamma values for the RBF SVM\n",
    "parameters_rbf = [{'kernel': ['rbf'], 'gamma': [10, 100, 1000], 'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "clf_rbf = GridSearchCV(svc, parameters_rbf, scoring = 'roc_auc')\n",
    "clf_rbf.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_rbf = pd.DataFrame(clf_rbf.cv_results_)\n",
    "res_rbf\n",
    "# RBF gamma = 10, C=10 is still the best followed by gamma = 10, C=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting RBF, gamma = 10, C=10 to get full score breakdown\n",
    "\n",
    "RBF_SVM = SVC(C = 10 , class_weight = 'balanced', kernel = 'rbf', gamma = 10)\n",
    "\n",
    "RBF_scores, RBF_stds = evaluation_metrics(Xnormalized, classifier = RBF_SVM, has_decision_function = True)\n",
    "# 0.88"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying uniform mapped data\n",
    "RBF_scores_uni, RBF_stds_uni = evaluation_metrics(Xuniform, classifier = RBF_SVM, has_decision_function = True)\n",
    "\n",
    "# 0.53 ROC-AUC. Horrible performance, less ROC-AUC than random classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out the score summaries for normalized data RBF \n",
    "RBF_scores\n",
    "# 0.8759 ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard deviation summary for normalized data RBF \n",
    "RBF_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by running a grid search for the best $C$ for the Linear Regression.. I choose completelly arbitrary parameters aiming to covers full parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Grid Search for C\n",
    "\n",
    "parameters_logistic = [{'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "clf_logistic = GridSearchCV(logistic, parameters_logistic, scoring = 'roc_auc')\n",
    "clf_logistic.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the results in a table\n",
    "res_logistic = pd.DataFrame(clf_logistic.cv_results_)\n",
    "res_logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best score\n",
    "print(clf_logistic.best_params_, \"ROC-AUC:\", clf_logistic.best_score_)\n",
    "# worse than that RBF SVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing out the optimal results\n",
    "log_C = LogisticRegression(C = 10, max_iter=1000)\n",
    "\n",
    "log_score, log_stds = evaluation_metrics(Xnormalized, classifier=log_C, has_decision_function=True)\n",
    "# 0.79 ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying uniform mapped data\n",
    "\n",
    "log_score_uni, log_stds_uni = evaluation_metrics(Xuniform, classifier=log_C, has_decision_function = True)\n",
    "\n",
    "# 0.71 ROC-AUC. Substantially worse performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Scores:\",log_score, \"S.d.:\", log_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression analysis showed that the optimal parameter C which regulates overfitting is 10. This implies that it is better to penalize less the complex model in the logistic regression case.\n",
    "The resulting ROC-AUC score is 0.7901 which is worse than that of RBF SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes classification requires to assume the distribution of the likelihood of the data. I will implement Gaussian and Bernoulli Naive Bayes since these distributions fit the data the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# specifying prior probabilities of classes\n",
    "prior = np.array([1821/3000, 1179/3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gaus_bayes = GaussianNB(priors = prior)\n",
    "\n",
    "gaus_bayes_score, gaus_bayes_stds = evaluation_metrics(Xnormalized, classifier = gaus_bayes)\n",
    "# 0.73 ROC-AUC. Poor performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ber_bayes = BernoulliNB()\n",
    "\n",
    "ber_bayes_score, ber_bayes_stds = evaluation_metrics(Xnormalized, classifier = ber_bayes)\n",
    "# 0.74 ROC-AUC. Slightly better but still poor performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes did not bring good resuls - Gaussian Naive Bayes yielded ROC-AUC OF 0.73 anf Bernoulli Bayes - 0.74. This is not suprising given the assumption of indepence of features from one another given the label of the class. It is obvious that the brightness of one pixel gives information about its neighbours given the class of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_summary = { \" \": [\"Perceptron\", \"LSVC\", \"RBF SVM\", \"Logistic\", \"Binomial Naive Bayes\"],\n",
    "                    \"ROC-AUC\": [perceptron_scores['roc'], LSVC_score['roc'], RBF_scores['roc'], log_score['roc'], ber_bayes_score['roc']],\n",
    "                      \"ROC-AUC s.d.\": [perceptron_stds['roc'], LSVC_std['roc'], RBF_stds['roc'], log_stds['roc'], ber_bayes_stds['roc']],\n",
    "                     \"PR-AUC\": [perceptron_scores['pr'], LSVC_score['pr'], RBF_scores['pr'], log_score['pr'], ber_bayes_score['pr']],\n",
    "                      \"PR-AUC s.d.\": [perceptron_stds['pr'], LSVC_std['pr'], RBF_stds['pr'], log_stds['pr'], ber_bayes_stds['pr']],\n",
    "                     \"Accuracy\": [perceptron_scores['accuracy'], LSVC_score['accuracy'], RBF_scores['accuracy'], log_score['accuracy'], ber_bayes_score['accuracy']],\n",
    "                      \"Accuracy s.d.\": [perceptron_stds['accuracy'], LSVC_std['accuracy'], RBF_stds['accuracy'], log_stds['accuracy'], ber_bayes_stds['accuracy']]\n",
    "                     }\n",
    "\n",
    "model_summary = pd.DataFrame(data_summary, columns = [' ','ROC-AUC','ROC-AUC s.d.','PR-AUC','PR-AUC s.d.', 'Accuracy','Accuracy s.d.'])\n",
    "\n",
    "model_summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the model comparison table above, an obvious and absolute leader is RBF Support Vector Machine with $C = \\gamma = 10$. The model has the highest score across the performance metrics acompanied by the lowest standard deviation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### 2 component PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting PCA with 2 components\n",
    "pca = PCA(n_components=2, random_state = 10)\n",
    "projected = pca.fit_transform(Xnormalized)\n",
    "\n",
    "# checking the resulting data dimensions\n",
    "print(Xtrain.shape)\n",
    "print(projected.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the two components\n",
    "plt.scatter(projected[:, 0], projected[:, 1],\n",
    "            c=Ytrain, edgecolor='none', alpha=0.5,\n",
    "            cmap=plt.cm.get_cmap('YlOrRd', 2))\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel( 'PC 2')\n",
    "plt.colorbar();\n",
    "# it seems that 1 is contained within -1 with 1 taking a slightly smaller area, \n",
    "# yet they are very similar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 1 class is contained within -1, i.e. - class -1 has a somehow higher range. Apart from that, the both classes are very similar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scree graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Starting  with the maximum of PCs for a scree plot\n",
    "pca_big = PCA(n_components=784, random_state = 10)\n",
    "pca_big.fit(Xnormalized)\n",
    "\n",
    "X_pca_big = pca_big.fit_transform(Xnormalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting scree map\n",
    "plt.plot(np.arange(len(pca_big.explained_variance_ratio_))+1,\n",
    "         np.cumsum(pca_big.explained_variance_ratio_),'o-', color='orange')\n",
    "plt.axis([1,len(pca_big.explained_variance_ratio_),0,1])\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.title('Scree Graph')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that 95% of variation is explained in the 100-200 PC bracket. Trying different values, I find that 165 PCs explain 95% of variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_165 = PCA(n_components=165, random_state = 10)\n",
    "pca_165.fit(Xnormalized)\n",
    "\n",
    "X_pca_165 = pca_165.fit_transform(Xnormalized)\n",
    "\n",
    "# calculating total variance explained\n",
    "sum(pca_165.explained_variance_ratio_)\n",
    "# so need 165 PCAs to explain 95% of variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernalized SVM on PCA data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by running a grid search for the best parameters of Kernalized SVM using the reduced PCA data. I choose completelly arbitrary parameters aiming to covers full parameter space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kernalized SVM on 165 PC data.\n",
    "\n",
    "parameters_pc = [{'kernel': ['poly'], 'degree': [2, 3, 4, 5], 'C': [0.01, 0.1, 1, 10, 100]},\n",
    "              {'kernel': ['rbf'], 'gamma': [1e-4, 1e-2, 1, 10], 'C': [0.01, 0.1, 1, 10, 100]}]\n",
    "\n",
    "svc = SVC(class_weight = 'balanced')\n",
    "pc_clf = GridSearchCV(svc, parameters_pc, scoring = 'roc_auc', cv=skf)\n",
    "pc_clf.fit(X_pca_165, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# printing the best results\n",
    "print(pc_clf.best_params_, \"ROC-AUC:\", pc_clf.best_score_)\n",
    "\n",
    "# The result is marginally lower than that of non-reduced data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most optimal kernel appears to be again RBF with C = 1 and $\\gamma = 10$. The corresponding ROC-AUC score (0.8731) is almost the same as that achieved with non-reduced data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overall, using 165 principal component versus the non-reduced data brings the loss of ROC-AUC score equal to approximatelly 0.00274. So we do not gain better classification performance, yet in the case of a huge data, PCA could help with computational efficiency. Nonetheless, in this particular case, in order to maximise classification, I would use the non-reduced data.\n",
    "\n",
    "The most optimal Support Vector Machine case, had a lower regulating term C which implies that more reguliarization is needed whilst using the PCA data. Overall, PCA data could be used as a replacement for SVM models due to the computational efficiency and a marginal loss in predictive power.\n",
    "\n",
    "The breakdown of the most optimal SVM model with RBF kernel, $\\gamma = 10$ and $C = 1$ is presented bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting RBF, gamma = 10, C=1 to get full score breakdown\n",
    "\n",
    "PC_SVM = SVC(C = 1 , class_weight = 'balanced', kernel = 'rbf', gamma = 10)\n",
    "\n",
    "PC_SVM_scores, PC_SVM_stds = evaluation_metrics(X_pca_165, classifier = PC_SVM, has_decision_function = True)\n",
    "# 0.87 ROC-AUC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Score:\", PC_SVM_scores, \"Stand. Dev:\", PC_SVM_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I start by running a grid search for the best parameters of XGBoosting using the reduced PCA data. I choose completelly arbitrary parameters aiming to covers full parameter space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search of XGBoosting parameters\n",
    "\n",
    "parameters_T = [{'colsample_bytree': [1, 0.6, 0.3], 'colsample_bylevel': [1, 0.6, 0.3], \n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'learning_rate': [0.1, 0.3], 'max_depth':[3, 6, 10, 12]}]\n",
    "\n",
    "xgb_T = xgb.XGBClassifier()\n",
    "xgb_T = GridSearchCV(xgb_T, parameters_T, scoring = 'roc_auc')\n",
    "xgb_T.fit(X_pca_165, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaping the results into the dataframe for a better inspection if needed.\n",
    "resxgb_T = pd.DataFrame(xgb_T.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best parameters with the corresponding ROC-AUC score\n",
    "print(xgb_T.best_params_, xgb_T.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most optimal XGBoosting parameters yields ROC-AUC of roughly 0.8798. This is the highest score so far exceeding even that of the RBF SVM with the non-reduced data. I would expect the score to increase further if the samee XGBoosting with the same parameters was applied on a normalized but non-reduced data. \n",
    "\n",
    "In order to improve the classification score even further, I conduct another one grid search. This time I am looking around the parameters which were optimal now. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Narrower grid search for XGBoosting\n",
    "\n",
    "parameters_T2 = [{'colsample_bytree': [1, 0.9, 0.8], 'colsample_bylevel': [0.7, 0.6, 0.5], \n",
    "                'n_estimators': [180, 200, 220],\n",
    "                'learning_rate': [0.1, 0.2], 'max_depth':[9, 10, 11]}]\n",
    "\n",
    "xgb_T2 = xgb.XGBClassifier()\n",
    "xgb_T2 = GridSearchCV(xgb_T2, parameters_T2, scoring = 'roc_auc')\n",
    "xgb_T2.fit(X_pca_165, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shaping the results into the dataframe for a better inspection if needed.\n",
    "resxgb_T2 = pd.DataFrame(xgb_T2.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing the best parameters with the corresponding ROC-AUC score\n",
    "print(xgb_T2.best_params_, xgb_T2.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score is even better. Setting XGBoosting with these parameters to get the full performance evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_class_def = xgb.XGBClassifier(colsample_bylevel=0.7, colsample_bytree=0.9,\n",
    "                                  learning_rate=0.1, max_depth=11, n_estimators=180)\n",
    "\n",
    "xgb_score, xgb_std = evaluation_metrics(X_pca_165, classifier=xgb_class_def)\n",
    "# 0.8807 ROC-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_score\n",
    "# 0.8807 ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the XGBoosting with colsample_bylevel = 0.7, colsample_bytree = 0.9, learning_rate = 0.1, \n",
    " max_depth = 11, n_estimators = 180 gave the best results on PCA data exceeding even the results obtained by RBF SVM on a full normalized data, for my pipeline I will implement the same XGBoosting algorithm yet on the full normalized data. The reasoning behind is that when comparing RBF SVM on PCA and full data, full data still yielded slightly better results.\n",
    "\n",
    "The pipeline is straightforward and follows two steps:\n",
    " 1. Normalize the data based on L2 norm\n",
    " 2. Fit XGBoosting classifier with colsample_bylevel = 0.7, colsample_bytree = 0.9, learning_rate = 0.1, max_depth = 11, n_estimators = 180 parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing Xtest data.\n",
    "Xtest_normalized = preprocessing.normalize(Xtest, norm='l2')\n",
    "\n",
    "# Fitting the XGBoosting model\n",
    "X_test_clsf = xgb.XGBClassifier(colsample_bylevel=0.7, colsample_bytree= 0.9, learning_rate= 0.1, \n",
    " max_depth= 11, n_estimators= 180)\n",
    "\n",
    "X_test_clsf.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the corresponding scores. \n",
    "Y_test_predict = np.array(X_test_clsf.predict_proba(Xtest_normalized)[:,1])\n",
    "\n",
    "# Changing printing options from the scientific notation to a simple decimal one. \n",
    "np.set_printoptions(suppress=True, precision=10)\n",
    "\n",
    "# saving the file in the directory. \n",
    "np.savetxt(\"two.csv\", Y_test_predict, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above gave the prediction score in the class competition of 0.90. I will also try the XGBoosting with the optimal parameters found before doing a narrower search because owing to randomness of the data, the score might be higher for another setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the XGBoosting model with previous optimal settings.\n",
    "\n",
    "X_test_clsf_2 = xgb.XGBClassifier(colsample_bylevel=0.6, colsample_bytree= 1, learning_rate= 0.1, \n",
    " max_depth= 10, n_estimators = 200)\n",
    "\n",
    "X_test_clsf_2.fit(Xnormalized, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the corresponding scores. \n",
    "Y_test_predict_2 = np.array(X_test_clsf_2.predict_proba(Xtest_normalized)[:,1])\n",
    "\n",
    "# Changing printing options from the scientific notation to a simple decimal one. \n",
    "np.set_printoptions(suppress=True, precision=10)\n",
    "\n",
    "# saving the file in the directory. \n",
    "np.savetxt(\"one.csv\", Y_test_predict_2, delimiter=\",\", fmt='%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This specification yielded a predictino score of 0.9010 which is slightly higher than the one before. I thus, leave these predicted scores. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
